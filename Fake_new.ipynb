{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nupurshivani/Nupurshivani/blob/main/Fake_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FAQs for the Image Embeddings Code:\n",
        "\n",
        "**Q1: What does the code block with `drive.mount('/content/drive')` do?**\n",
        "*This line mounts Google Drive to the Colab environment, allowing access to files and folders stored in Google Drive.*\n",
        "\n",
        "**Q2: Why are the `ResNet50` model and libraries from TensorFlow used in this code?**\n",
        "*The code uses the ResNet50 model from TensorFlow's Keras applications for image feature extraction. ResNet50 is a pre-trained convolutional neural network (CNN) that is commonly used for image-related tasks.*\n",
        "\n",
        "**Q3: How is the pre-trained ResNet50 model loaded, and why is the last layer removed?**\n",
        "*The pre-trained ResNet50 model is loaded using `ResNet50()` from Keras. The last layer is removed by creating a new model (`model`) that takes the same input as the original model but outputs from the second-to-last layer.*\n",
        "\n",
        "**Q4: What is the purpose of the `generate_embeddings` function?**\n",
        "*The `generate_embeddings` function takes an image path as input, preprocesses the image using ResNet50-specific preprocessing, and generates embeddings (features) using the truncated ResNet50 model.*\n",
        "\n",
        "**Q5: Why is the image array reshaped to have an additional dimension before passing it to the model?**\n",
        "*The additional dimension is added to simulate a batch of images. Although only one image is processed at a time, the model expects input in batches. This is achieved by adding a singleton dimension.*\n",
        "\n",
        "**Q6: How is the image folder specified, and what kind of images are processed?**\n",
        "*The variable `folder_path` specifies the path to the folder containing images in Google Drive. The code processes images with file extensions '.jpg', '.jpeg', and '.png'.*\n",
        "\n",
        "**Q7: What does the loop through each image in the folder do?**\n",
        "*The loop iterates through each file in the specified folder, and for each image file, it generates embeddings using the `generate_embeddings` function and stores the filename and embeddings in a DataFrame.*\n",
        "\n",
        "**Q8: How is the resulting DataFrame structured, and what does `print(df)` display?**\n",
        "*The DataFrame has two columns: 'Image Name' and 'Embeddings'. The 'Image Name' column contains the filenames, and the 'Embeddings' column contains the corresponding embeddings. `print(df)` displays the DataFrame.*\n",
        "\n",
        "**Q9: Why is there commented-out code for saving the DataFrame to a CSV file?**\n",
        "*The commented-out code shows how to save the DataFrame to a CSV file. It is currently disabled, but you can uncomment it to save the results to a CSV file.*\n",
        "\n",
        "**Q10: How can I use the generated embeddings for downstream tasks?**\n",
        "*The embeddings can be used as input features for various machine learning tasks. For example, you can use them for image similarity, clustering, or as input features for a classification model.*"
      ],
      "metadata": {
        "id": "Yg7kGozWZYDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load the pre-trained ResNet50 model\n",
        "# base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "base_model = VGG16()\n",
        "\n",
        "\n",
        "# Get the output from the second-to-last layer\n",
        "outputs = base_model.layers[-2].output\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# Function to preprocess an image and generate embeddings\n",
        "def generate_embeddings(image_path):\n",
        "    img = image.load_img(image_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    # img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array.reshape((1, img_array.shape[0], img_array.shape[1], img_array.shape[2]))\n",
        "\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    embeddings = model.predict(img_array)\n",
        "    return embeddings\n",
        "\n",
        "# Specify the path to the folder containing images in Google Drive\n",
        "folder_path = '/content/images2-20231225T105040Z-001.zip'\n",
        "\n",
        "# Create a DataFrame to store image names and embeddings\n",
        "data = {'Image Name': [], 'Embeddings': []}\n",
        "\n",
        "# Loop through each image in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "        image_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Generate embeddings for the image\n",
        "        image_embeddings = generate_embeddings(image_path)\n",
        "\n",
        "        # Store the results in the DataFrame\n",
        "        data['Image Name'].append(filename)\n",
        "        data['Embeddings'].append(image_embeddings)\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "\n",
        "# # Save the DataFrame to a CSV file\n",
        "# csv_file_path = '/content/drive/MyDrive/fake-news-hands-on/image_embeddings.csv'\n",
        "# df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# print(\"Embeddings saved to:\", csv_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "8iyCaG1W8QZa",
        "outputId": "121f7b38-d10c-4d7f-854b-ad569754c4dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 7s 0us/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0ae4336935aa>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Loop through each image in the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/images2-20231225T105040Z-001.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first embedding from the DataFrame\n",
        "print(\"First Embedding:\")\n",
        "print(df['Embeddings'][0].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KvXnyc9_dH5",
        "outputId": "86a9b4f3-34fa-49b8-8a28-5ed7372c4c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Embedding:\n",
            "(1, 2048)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ResNet50 Overview:**\n",
        "\n",
        "**1. Introduction:**\n",
        "   - **ResNet50 (Residual Network):** ResNet50 is a deep convolutional neural network architecture that was introduced by Microsoft Research in 2015. It is a part of the ResNet family, which is known for its success in training very deep neural networks.\n",
        "\n",
        "**2. Key Features:**\n",
        "   - **Deep Architecture:** ResNet50 is particularly notable for its depth. It has 50 layers, and the architecture is designed to handle the challenges associated with training very deep networks.\n",
        "   - **Skip Connections (Residual Blocks):** ResNet50 introduces the concept of residual learning, where the network learns residual functions with respect to the layer inputs. This is implemented through skip connections, allowing information to bypass one or more layers.\n",
        "\n",
        "**3. Architecture Details:**\n",
        "   - **Convolutional Layers:** ResNet50 primarily consists of convolutional layers with small 3x3 filters. The depth of the network is achieved through the stacking of these layers.\n",
        "   - **Residual Blocks:** Residual blocks are the building blocks of ResNet. Each block includes multiple convolutional layers along with shortcut connections. The output of the block is the sum of the input and the output of the convolutional layers.\n",
        "   - **Bottleneck Design:** ResNet50 uses a bottleneck design in residual blocks, which involves 1x1, 3x3, and 1x1 convolutions. This design helps in reducing the computational cost.\n",
        "\n",
        "**4. Pre-training and Transfer Learning:**\n",
        "   - **ImageNet Pre-training:** ResNet50 is often pre-trained on large datasets like ImageNet. Pre-training enables the model to learn general features from a diverse range of images.\n",
        "   - **Transfer Learning:** Due to its pre-training on ImageNet, ResNet50 is commonly used for transfer learning. Researchers and practitioners often use the pre-trained ResNet50 as a feature extractor for various computer vision tasks.\n",
        "\n",
        "**5. Applications:**\n",
        "   - **Image Classification:** ResNet50 is frequently used for image classification tasks, where it excels in providing accurate predictions on a diverse set of images.\n",
        "   - **Object Detection:** The architecture is also utilized in object detection frameworks. It can be employed as a backbone network in detectors like Faster R-CNN.\n",
        "   - **Feature Extraction:** ResNet50 serves as an effective feature extractor in various downstream tasks, including image similarity, clustering, and more.\n",
        "\n",
        "**6. TensorFlow and Keras Implementation:**\n",
        "   - **Availability:** ResNet50 is available as part of TensorFlow's Keras applications module. It can be easily imported and used for various computer vision tasks.\n",
        "\n",
        "**7. Considerations:**\n",
        "   - **Computational Cost:** While ResNet50 is powerful, it comes with a significant computational cost due to its depth. Training large models like ResNet50 may require substantial computational resources.\n",
        "\n",
        "**8. Further Reading:**\n",
        "   - **Original Paper:** \"Deep Residual Learning for Image Recognition\" by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. (ArXiv, 2015) [Link](https://arxiv.org/abs/1512.03385)\n",
        "\n",
        "ResNet50 has become a foundational architecture in the field of computer vision, and its principles of residual learning have influenced the design of subsequent deep neural networks."
      ],
      "metadata": {
        "id": "2L2pVgQUZbYw"
      }
    }
  ]
}